# 6.3 파일을 읽을 때 프로그램에는 어떤 일이 발생할까?

파일 읽기 동작은 메모리 관점에서 보면 외부 장치와 메모리 사이의 데이터 복사 과정이라고 이해할 수 있다.

입출력은 단순히 화면에 출력하는 행위에 그치지 않고, 메모리와 디스크·네트워크 카드·화면 등의 외부 장치 사이에서 데이터가 이동하는 모든 과정을 의미한다.

파일을 읽는다는 것은 디스크에 저장된 데이터를 프로세스의 주소 공간으로 가져오는 과정이며, 이 과정에는 운영 체제, CPU, 디스크, 메모리, 스케줄링이 모두 관여한다.

---

## 6.3.1 메모리 관점에서 입출력

입출력은 메모리의 복사(copy) 과정이라고 볼 수 있다.

외부 장치에서 메모리로 데이터가 복사되면 입력(input)이라 하고, 메모리에서 외부 장치로 복사되면 출력(output)이라 한다.

이처럼 메모리와 외부 장치 사이에서 데이터가 오가는 과정을 I/O라고 한다.

파일을 읽는 경우를 예로 들면, 디스크에 있는 데이터가 프로세스의 메모리 공간으로 복사된다.

이때 프로그래머는 단순히 `read()`와 같은 함수를 호출하지만, 실제 내부에서는 다음과 같은 일이 발생한다.

- 프로세스는 데이터를 저장할 버퍼를 준비한다.
- `read()` 호출은 시스템 호출을 통해 운영 체제에 파일 읽기를 요청한다.
- 운영 체제는 디스크에 읽기 명령을 전달한다.

디스크는 CPU보다 매우 느리기 때문에, 입출력 요청이 완료될 때까지 프로세스를 계속 실행시키면 CPU 자원이 낭비된다.

따라서 운영 체제는 해당 프로세스를 입출력 블로킹 대기열에 넣고 일시 중지시킨다.

이때 디스크는 DMA(Direct Memory Access) 방식을 사용해 CPU를 거치지 않고 디스크 데이터를 메모리의 지정된 버퍼로 직접 복사한다.

데이터 복사가 완료되면 디스크는 CPU에 인터럽트 신호를 보내 작업 완료를 알린다.

운영 체제는 인터럽트를 처리한 뒤, 해당 프로세스를 입출력 블로킹 대기열에서 꺼내 준비 완료 대기열로 이동시킨다.

---

## 6.3.2 read 함수는 어떻게 파일을 읽는 것일까?

단일 코어 CPU 시스템에서 프로세스 A와 프로세스 B가 존재한다고 가정한다.

프로세스 A가 `read()`를 호출하면 다음과 같은 흐름이 발생한다.

1. 프로세스 A가 시스템 호출을 통해 파일 읽기를 요청한다.
2. 운영 체제가 디스크에 읽기 명령을 전달한다.
3. 프로세스 A는 입출력 블로킹 상태로 전환되어 대기열에 들어간다.
4. CPU는 준비 완료 대기열에 있는 다른 프로세스 B를 실행한다.
5. 디스크는 DMA를 통해 데이터를 프로세스 A의 버퍼에 복사한다.
6. 복사가 완료되면 디스크는 인터럽트를 발생시킨다.
7. 운영 체제는 인터럽트를 처리하고 프로세스 A를 준비 완료 상태로 전환한다.
8. 스케줄러는 적절한 시점에 프로세스 A를 다시 CPU에 할당한다.

이 과정에서 중요한 점은 다음과 같다.

- 입출력 작업 동안 CPU는 놀지 않고 다른 프로세스를 실행한다.
- 디스크와 CPU는 병렬적으로 동작하여 자원을 효율적으로 활용한다.
- 프로세스는 자신이 중간에 중지되었다는 사실을 거의 인지하지 못한다.
- 운영 체제의 스케줄링이 CPU와 디스크의 활용도를 극대화한다.

또한 실제 시스템에서는 디스크 데이터가 곧바로 프로세스의 주소 공간으로 복사되지 않고, 먼저 운영 체제 내부 버퍼로 복사된 후 다시 프로세스 메모리로 복사되는 경우가 일반적이다.

이처럼 복사 단계가 한 번 더 존재할 수 있다.

운영 체제를 거치지 않고 직접 프로세스 주소 공간으로 복사하는 방식을 무복사(zero-copy) 기법이라고 한다.

---

# 6.4 높은 동시성의 비결: 입출력 다중화

유닉스/리눅스에서는 파일을 바이트의 연속된 수열로 추상화한다.

디스크 파일, 네트워크 소켓, 터미널, 파이프 등 대부분의 입출력 장치를 파일로 다룬다. 이를 “모든 것은 파일이다”라고 표현한다.

모든 입출력 작업은 파일 열기, 읽기, 쓰기, 위치 이동, 닫기 같은 공통 인터페이스로 처리한다.

`open`, `read`, `write`, `seek`, `close` 함수만으로 다양한 외부 장치를 동일한 방식으로 제어한다.

이 추상화 덕분에 프로그래머는 장치의 내부 구현을 몰라도 일관된 방식으로 입출력을 다룰 수 있다.

---

## 6.4.1 파일 서술자

유닉스/리눅스에서 파일을 사용하려면 파일 서술자(file descriptor)를 발급받는다.

파일 서술자는 커널이 반환하는 정수 값이며, 열린 파일을 식별하는 번호이다.

- `open()`을 호출하면 커널은 파일 서술자를 반환한다.
- 이후 `read(fd, buffer)`와 같이 파일 서술자를 인자로 전달하여 입출력을 수행한다.
- 커널은 파일 서술자를 통해 해당 파일과 관련된 모든 내부 정보를 찾아 처리한다.

프로세스는 파일이 디스크 어디에 저장되어 있는지, 현재 읽기 위치가 어디인지 등을 알 필요가 없다.

이 모든 정보는 커널이 관리하며, 프로세스는 파일 서술자라는 숫자만 사용하면 된다.

파일 서술자는 메모리, 키보드, 화면, 디스크 등 다양한 장치에 대해 동일하게 적용된다.

이로써 프로그램은 장치 종류에 관계없이 일관된 방식으로 입출력을 수행한다.

---

## 6.4.2 다중 입출력을 어떻게 효율적으로 처리하는 것일까?

높은 동시성은 서버가 동시에 많은 사용자 요청을 처리하는 능력을 의미한다.

네트워크 통신 역시 파일 서술자를 기반으로 동작하며, `accept()` 호출을 통해 연결을 수락하면 새로운 파일 서술자를 얻는다.

일반적인 서버 처리 흐름은 다음과 같다.

- `accept()`로 사용자 연결을 수락한다.
- 반환된 파일 서술자를 통해 `read()`로 요청 데이터를 읽는다.
- 데이터를 기반으로 필요한 처리를 수행한다.

문제는 `read()`가 기본적으로 블로킹 호출이라는 점이다.

특정 파일 서술자에서 데이터가 준비되지 않은 상태에서 `read()`를 호출하면 해당 스레드는 대기 상태가 된다.

여러 사용자 요청을 순차적으로 처리하면 다음과 같은 문제가 발생한다.

- 첫 번째 사용자의 데이터가 도착하지 않으면 스레드 전체가 멈춘다.
- 두 번째 사용자가 이미 데이터를 보냈더라도 처리하지 못한다.
- 사용자 수가 많아질수록 이러한 구조는 비효율적이 된다.

이를 해결하기 위해 사용자마다 스레드를 생성하는 방법을 생각할 수 있다.

그러나 스레드 수가 많아지면 문맥 전환 비용과 스케줄링 부담이 증가하여 확장성이 떨어진다.

따라서 핵심 문제는 다음과 같다.

- 어떤 파일 서술자가 읽기 또는 쓰기 가능한 상태인지 미리 알 수 있어야 한다.
- 준비되지 않은 파일 서술자에 대해 블로킹 호출을 하지 않아야 한다.

---

## 6.4.3 상대방이 아닌 내가 전화하게 만들기

입출력 다중화의 핵심 아이디어는 커널이 준비 상태를 알려주도록 만드는 것이다.

기존 방식은 다음과 같다.

- 프로그램이 각 파일 서술자에 대해 “읽을 수 있는가?”를 반복적으로 확인한다.
- 준비되지 않은 상태에서 호출하면 블로킹된다.

개선된 방식은 다음과 같다.

- 관심 있는 파일 서술자 목록을 커널에 등록한다.
- 커널이 해당 파일 서술자 중 읽기/쓰기 가능한 것이 생기면 프로그램에 알려준다.
- 프로그램은 준비된 파일 서술자만 처리한다.

이 방식은 마치 상대방이 전화를 걸어주기를 기다리는 것이 아니라, 준비되었을 때만 연락을 받는 구조와 유사하다.

이로써 프로그램은 불필요한 대기와 반복 확인을 하지 않아도 된다.

이것이 입출력 다중화(input/output multiplexing) 기술이다.

---

## 6.4.4 입출력 다중화

다중화(multiplexing)는 여러 신호를 하나의 채널로 합치는 기술을 의미한다.

입출력 다중화는 여러 파일 서술자를 하나의 흐름에서 효율적으로 감시하고 처리하는 기법이다.

입출력 다중화의 일반적인 흐름은 다음과 같다.

1. 여러 파일 서술자를 확보한다.
2. 감시 대상 파일 서술자 집합을 커널에 등록한다.
3. 읽기 또는 쓰기 가능한 파일 서술자가 발생할 때까지 대기한다.
4. 준비된 파일 서술자만 선택하여 처리한다.

이 기법을 사용하면 하나의 스레드로도 여러 소켓이나 파일을 동시에 다룰 수 있다.

리눅스에서 대표적인 입출력 다중화 기법은 `select`, `poll`, `epoll`이다.

---

## 6.4.5 삼총사: select, poll, epoll

`select`, `poll`, `epoll`은 모두 동기 방식의 입출력 다중화 기술이다.

지정한 파일 서술자 집합에서 이벤트가 발생할 때까지 대기한 후, 준비된 파일 서술자를 알려준다.

### select

- 감시 가능한 파일 서술자 수에 제한이 있다.
- 내부적으로 모든 파일 서술자를 반복 검사한다.
- 감시 대상이 많아질수록 성능이 저하된다.
- 호출 시마다 파일 서술자 집합을 다시 전달해야 한다.

### poll

- `select`의 파일 서술자 개수 제한을 개선한다.
- 그러나 감시 대상 수가 많아질수록 여전히 선형 탐색 비용이 발생한다.
- 대규모 동시성 환경에서는 성능 한계가 있다.

### epoll

- 리눅스에서 고성능 서버를 위해 도입된 방식이다.
- 커널 내부에 별도의 자료 구조를 유지한다.
- 준비 완료된 파일 서술자 목록을 별도로 관리한다.
- 이벤트가 발생한 파일 서술자만 반환하므로 효율적이다.
- 대규모 연결 환경에서 높은 성능을 제공한다.

`epoll`은 많은 네트워크 프레임워크와 서버 라이브러리에서 기본적으로 사용된다.

---

# 6.5 mmap: 메모리 읽기와 쓰기 방식으로 파일 처리하기

mmap은 파일을 메모리처럼 다루기 위한 메커니즘이다.

운영체제의 가상 메모리 기능을 활용하여 디스크 파일을 프로세스 주소 공간에 직접 사상한다.

기존의 read/write 방식과 비교하여 동작 원리와 장단점을 이해하는 것이 중요하다.

---

## 6.5.1 파일과 가상 메모리

메모리를 읽고 쓰는 작업은 매우 단순하다. 배열을 선언하고 특정 인덱스에 값을 대입하면 곧바로 메모리에 저장된다. 이 과정에서 운영체제 개입을 의식하지 않는다.

반면 파일을 읽고 쓰기 위해서는 open으로 파일을 열고 파일 서술자를 얻은 뒤 read/write 시스템 호출을 수행해야 한다. 이는 커널을 거치는 과정이므로 복잡하다.

그 이유는 메모리와 디스크의 주소 지정 방식과 속도가 다르기 때문이다. 메모리는 바이트 단위로 직접 주소를 지정하지만 디스크는 블록 단위로 접근한다. 또한 CPU와 디스크의 속도 차이가 매우 크다.

파일을 메모리처럼 다루기 위해서는 두 공간을 연결하는 장치가 필요하며, 그 역할을 가상 메모리가 수행한다.

가상 메모리는 프로세스에게 연속된 주소 공간을 제공한다. 이 주소 공간에 파일을 직접 사상하면 파일을 메모리처럼 접근할 수 있다.

---

## 6.5.2 마술사 운영 체제

mmap은 파일을 프로세스의 가상 주소 공간에 사상한다. 예를 들어 200바이트 파일을 주소 공간 600~799 범위에 매핑하면, 해당 주소에 접근하는 행위는 곧 파일에 접근하는 행위가 된다.

프로세스가 매핑된 영역에 처음 접근하면 페이지 누락(page fault)이 발생한다. CPU는 인터럽트를 통해 운영체제로 제어를 넘긴다.

운영체제는 해당 파일의 내용을 디스크에서 읽어 물리 메모리에 적재하고 가상 주소와 물리 주소의 매핑을 설정한다. 이후 프로그램은 아무 일도 없었던 것처럼 계속 실행된다.

쓰기 작업 또한 동일하게 처리된다. 프로세스는 메모리를 수정하듯이 값을 변경하며, 운영체제는 이를 적절한 시점에 디스크에 반영한다.

응용 프로그램은 디스크 I/O를 직접 처리하지 않고도 파일을 메모리처럼 사용할 수 있다.

---

## 6.5.3 mmap 대 전통적인 read/write 함수

read/write 함수는 시스템 호출을 통해 커널로 진입한다. read는 디스크에서 데이터를 커널 버퍼로 읽은 뒤 사용자 공간 버퍼로 복사한다. write는 사용자 공간에서 커널 공간으로 데이터를 복사한 뒤 디스크에 기록한다.

이 과정에서 시스템 호출 비용과 데이터 복사 비용이 발생한다.

mmap은 파일을 가상 주소 공간에 직접 사상하므로 명시적인 데이터 복사가 필요하지 않다. 따라서 복사에 따른 오버헤드를 줄일 수 있다.

그러나 mmap 역시 페이지 누락 인터럽트 비용과 커널의 매핑 관리 비용이 존재한다. 커널은 프로세스 주소 공간과 파일의 사상 관계를 유지해야 하므로 내부 자료구조 관리 비용이 발생한다.

따라서 mmap이 항상 read/write보다 빠르다고 단정할 수 없다. 실제 성능은 접근 패턴과 상황에 따라 달라진다.

---

## 6.5.4 큰 파일 처리

전통적인 read/write 방식으로 큰 파일을 처리할 경우 파일을 여러 번 나누어 읽어야 한다. 전체 파일을 한 번에 메모리에 올리기 어렵기 때문이다.

물리 메모리보다 큰 파일을 처리할 때는 메모리 부족 문제가 발생할 수 있으며, 잘못하면 out-of-memory 상황이 발생한다.

mmap을 사용하면 가상 메모리 덕분에 파일 전체를 주소 공간에 사상할 수 있다. 실제 물리 메모리보다 큰 파일이라도 문제없이 다룰 수 있다.

필요한 부분만 페이지 단위로 메모리에 적재되므로 메모리 사용이 효율적이다.

MAP_SHARED 옵션을 사용하면 수정 내용이 파일에 직접 반영된다. MAP_PRIVATE 옵션을 사용하면 복사본에 대해 수정이 이루어진다.

32비트 시스템은 주소 공간이 4GB로 제한되므로 큰 파일 매핑에 제약이 있다. 64비트 시스템은 주소 공간이 충분히 넓어 이러한 제약이 크게 완화된다.

---

## 6.5.5 동적 링크 라이브러리와 공유 메모리

동적 링크 라이브러리는 mmap의 대표적인 활용 사례이다.

정적 링크는 라이브러리 코드를 실행 파일에 복사한다. 여러 프로그램이 동일한 라이브러리를 사용하면 메모리와 디스크 공간이 중복 사용된다.

동적 링크는 라이브러리를 별도의 파일로 두고 실행 시 로드한다. 여러 프로세스가 동일한 라이브러리를 참조하더라도 실제 물리 메모리에는 하나의 사본만 존재한다.

이는 mmap을 이용해 라이브러리 파일을 각 프로세스의 주소 공간에 사상하기 때문에 가능하다.

각 프로세스는 자신의 주소 공간에 라이브러리가 존재한다고 생각하지만, 실제 물리 메모리는 공유된다. 이로 인해 메모리 사용량을 크게 절감할 수 있다.

---

## 6.5.6 mmap 직접 조작하기

strace 명령을 사용하면 프로그램 실행 과정에서 호출되는 시스템 호출을 확인할 수 있다.

ls와 같은 프로그램을 strace로 추적해 보면 실행 초기에 여러 동적 라이브러리가 open되고 mmap으로 사상되는 것을 확인할 수 있다.

ld.so.cache 파일을 통해 필요한 동적 라이브러리를 찾고, 이후 해당 라이브러리 파일을 열어 mmap으로 주소 공간에 매핑한다.

libc.so와 같은 표준 C 라이브러리 역시 동일한 방식으로 매핑된다. 거의 모든 프로그램은 실행 시 여러 동적 라이브러리를 mmap을 통해 로드한다.

이는 mmap이 현대 운영체제에서 매우 핵심적인 메커니즘임을 보여준다.

---

# 6.6 컴퓨터 시스템의 각 부분에서 얼마큼 지연이 일어날까?

컴퓨터 시스템의 각 구성 요소는 서로 다른 수준의 지연 시간을 가진다.

이 지연 시간의 차이를 이해하는 것은 시스템 설계와 성능 최적화에 매우 중요하다.

제프 딘(Jeff Dean)이 정리한 통계를 기반으로 CPU, 메모리, 네트워크, 디스크 등에서 발생하는 지연 시간을 비교한다.

---

## 6.6.1 시간 지표로 환산

컴퓨터 세계의 시간 단위인 ns, μs, ms는 직관적으로 체감하기 어렵다.

따라서 L1 캐시 접근 시간 0.5ns를 1초로 가정하여 상대적인 지연을 비교한다.

이 기준으로 환산하면 다음과 같은 차이가 발생한다.

- L1 캐시 접근은 1초에 해당한다.
- 메모리 접근은 약 3분에 해당한다.
- 1MB 데이터를 메모리에서 읽는 시간은 약 5일에 해당한다.
- SSD에서 1MB를 읽는 시간은 약 20일에 해당한다.
- 디스크에서 1MB를 읽는 시간은 약 1년에 해당한다.
- 물리 시스템 재부팅은 약 5600년에 해당한다.

이 비교를 통해 캐시와 메모리, SSD, 디스크, 네트워크 사이의 지연 차이가 극단적으로 크다는 사실을 직관적으로 이해할 수 있다.

특히 메모리와 디스크의 차이는 수십 배에서 수백 배 이상에 달한다. 이는 디스크 접근을 최소화해야 하는 이유를 명확히 보여준다.

---

## 6.6.2 거리 지표로 환산

이번에는 시간을 거리로 환산하여 비교한다.

0.5ns를 1m로 가정하여 각 지연 시간을 거리로 변환한다.

환산 결과는 다음과 같은 감각적 차이를 보여준다.

- L1 캐시 접근은 집 안에서 몇 걸음 이동하는 거리와 같다.
- 메모리 접근은 집에서 편의점까지 다녀오는 거리 수준이다.
- 메모리에서 1MB를 읽는 시간은 서울에서 제주도까지 이동하는 거리 수준이다.
- 데이터센터 내부 네트워크 왕복은 서울에서 일본까지 이동하는 거리 수준이다.
- SSD에서 1MB를 읽는 시간은 서울에서 홍콩까지 이동하는 거리 수준이다.
- 디스크에서 1MB를 읽는 시간은 지구를 한 바퀴 도는 거리 수준이다.
- 대륙 간 네트워크 전송은 지구에서 달까지의 거리 수준이다.
- 시스템 재부팅은 지구에서 화성까지의 거리 수준이다.

이러한 환산은 지연 시간이 단순한 숫자 차이가 아니라 물리적으로 엄청난 규모 차이를 가진다는 점을 강조한다.

---

## 6.6.3 캐시와 메모리 계층의 의미

L2 캐시 접근 시간은 L1 캐시보다 약 14배 느리다.

메모리 접근은 L2 캐시보다 약 20배, L1 캐시보다 약 200배 느리다.

이 수치는 CPU와 메모리 사이에 캐시 계층이 존재하는 이유를 설명한다.

메모리 접근이 매우 느리기 때문에 캐시를 통해 접근 시간을 단축한다.

현대 CPU는 파이프라인 구조로 동작하므로 분기 예측 실패가 발생하면 이미 실행된 명령이 무효화된다.

분기 예측 실패 또한 ns 단위이지만 반복되면 성능에 큰 영향을 준다.

---

## 6.6.4 디스크와 SSD의 차이

동일한 1MB 데이터를 순차적으로 읽을 때의 지연 시간은 다음과 같다.

- 메모리는 약 250,000ns가 걸린다.
- SSD는 약 1,000,000ns가 걸린다.
- 디스크는 약 20,000,000ns가 걸린다.

즉, 디스크는 SSD보다 약 20배 느리고, 메모리보다 약 80배 느리다.

SSD는 메모리보다 약 4배 느리다.

특히 디스크의 탐색 시간(seek time)은 ms 단위로 매우 크다.

임의 접근(random read)은 탐색 비용이 반복적으로 발생하므로 매우 비효율적이다.

따라서 고성능 데이터베이스는 임의 접근을 줄이고 순차 기록(append) 방식을 선호한다.

---

## 6.6.5 네트워크 지연의 의미

같은 데이터센터 내부 네트워크 왕복도 메모리 접근에 비하면 매우 느리다.

대륙 간 네트워크 전송은 수백 ms 수준으로 더욱 크다.

이는 분산 시스템 설계에서 네트워크 호출을 최소화해야 하는 이유를 설명한다.

함수 호출과 네트워크 호출은 근본적으로 다른 비용 구조를 가진다.
