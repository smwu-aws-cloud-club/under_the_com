# 5. 작은 것으로 큰 성과 이루기, 캐시

## 5.1 캐시, 어디에나 존재하는 것

<img width="888" height="476" alt="image" src="https://github.com/user-attachments/assets/e4dac53b-64b6-4545-8ee9-c6ae0d6fc80e" />


폰 노이만 구조에서 CPU가 실행하는 프로그램은 결국 “메모리에 저장된 명령어와 데이터를 CPU가 가져와 처리하는 과정”으로 귀결된다. CPU는 명령어를 가져와 해석하고 실행하며, 실행 과정에서 필요한 데이터도 메모리에서 읽고 결과를 다시 메모리에 쓴다. 즉, CPU가 아무리 똑똑해도 메모리에 접근하지 못하면 실행 자체가 성립하지 않는다.

하지만 문제는 CPU가 메모리에 접근하는 빈도가 지나치게 높고, 그 접근 속도가 CPU의 연산 속도를 따라가지 못한다는 점이다. 레지스터는 매우 빠르지만 용량이 작아서 모든 데이터를 넣어둘 수 없고, 결국 CPU는 자주 메모리를 들락날락할 수밖에 없다. 이때 메모리가 느리면 CPU는 계산을 하고 싶어도 기다리는 시간이 길어져 성능이 급격히 떨어진다. 이 구조적 병목을 해결하기 위해 등장하는 것이 캐시(cache)다.

캐시는 단순히 “중간에 하나 더 끼워 넣은 저장소”가 아니라, 현대 컴퓨터 성능을 결정하는 핵심 설계 철학에 가깝다. CPU가 메모리를 직접 상대하는 대신, CPU 가까이에 더 빠른 저장소를 두고 거기서 대부분의 접근을 해결하도록 만든다. 여기서 중요한 전제가 하나 있다. 실제 프로그램은 완전히 랜덤하게 데이터를 쓰지 않고, 최근에 쓴 데이터나 근처 데이터를 다시 쓸 가능성이 높다. 캐시는 이런 프로그램의 특성을 적극적으로 이용한다.

- 캐시는 CPU와 메모리 사이에 놓인 “빠르지만 작은” 저장소이다.
- 캐시의 목적은 “메모리 접근을 줄이는 것”이라기보다 “느린 메모리를 보이지 않게 만드는 것”에 가깝다.

---

## 5.1.1 CPU와 메모리의 속도 차이

<img width="700" height="357" alt="image" src="https://github.com/user-attachments/assets/cfe0e8cd-c7ef-4336-9d5c-f456efcb39a7" />


시스템 성능을 생각할 때 CPU와 메모리를 하나의 묶음으로 보면, 실제 성능은 언제나 더 느린 쪽에 의해 제한된다. 이를 설명할 때 책에서는 무통의 원리(cannikin law) 비유를 사용한다. 나무통이 여러 판자로 이루어져 있으면 물은 가장 짧은 판자 높이까지만 찰 수 있듯이, 시스템도 CPU가 아무리 빨라도 메모리가 느리면 전체 성능은 메모리에 의해 제한된다는 의미다.

현실에서는 시간이 갈수록 CPU와 메모리의 속도 격차가 벌어졌다. CPU는 미세공정 발전과 설계 기법 개선으로 성능이 빠르게 증가했지만, 메모리는 그 증가 폭이 상대적으로 작았다. 이 격차는 “조금 더 기다리면 해결될 문제”가 아니라, 구조적으로 계속 누적되는 문제다. 그래서 캐시는 성능을 올리는 보조 장치가 아니라, CPU가 제 속도로 일하기 위해 필요한 필수 장치가 된다.

- CPU가 빠를수록 메모리 병목은 더 심해진다.
- 메모리 병목이 심해질수록 캐시의 가치가 더 커진다.

---

## 5.1.2 도서관, 책상, 캐시

캐시는 “자주 쓰는 것은 가까운 곳에 둔다”는 직관을 컴퓨터 구조에 적용한 것이다. 도서관에서 필요한 책을 매번 서가로 가서 찾으면 오래 걸리지만, 자주 참고하는 책을 책상 위에 올려두면 훨씬 빠르게 작업할 수 있다. 여기서 서가는 메모리, 책상은 캐시에 해당한다.

이 비유에서 핵심은 “한 번 올려둔 책을 다시 볼 확률이 높다”는 점이다. 실제 프로그램도 비슷하게 동작한다. 반복문 안에서 같은 배열을 여러 번 접근하거나, 함수 호출이 반복되면서 비슷한 코드 영역이 계속 실행되거나, 특정 객체의 필드를 연달아 읽는 등 “최근에 접근한 것”을 다시 접근하는 패턴이 많다. 캐시는 이런 성질을 이용해 CPU가 메모리를 기다리는 시간을 줄인다.

단, 캐시는 용량이 작기 때문에 항상 필요한 데이터가 들어있을 수는 없다. 그래서 캐시의 효과는 “캐시에 원하는 데이터가 있느냐”에 의해 결정된다.

- 캐시에 있으면: 빠르게 처리된다. (캐시 히트)
- 캐시에 없으면: 메모리까지 내려가야 한다. (캐시 미스)

---

## 5.1.3 캐시는 계층 구조로 존재한다

<img width="272" height="186" alt="image" src="https://github.com/user-attachments/assets/0a82e5f3-a8cb-46c8-929f-4f7c1b4f01ce" />


현대 CPU에서 캐시는 보통 한 단계가 아니라 여러 단계로 존재한다. CPU에 가까울수록 더 빠르고 더 비싸고 더 작으며, CPU에서 멀어질수록 느려지고 용량이 커진다. 그래서 L1, L2, L3 같은 계층 구조가 만들어진다.

CPU가 어떤 데이터를 읽으려 하면 보통 아래 순서로 찾는다.

L1 → L2 → L3 → 메모리

L1에서 찾으면 가장 빠르지만, L1은 용량이 작다. L2는 L1보다 느리지만 더 크고, L3는 더 크지만 더 느리다. 이런 계층 구조를 쓰는 이유는 “속도와 용량은 동시에 크게 만들기 어렵다”는 현실적인 제약 때문이다. 빠른 메모리는 제조 비용이 크고 면적을 많이 차지하며 전력도 많이 소모한다. 그래서 빠른 메모리를 적게 두고, 느린 메모리를 많이 두는 방식으로 균형을 잡는다.

- L1: 가장 빠르다. CPU와 거의 붙어 있는 느낌이다.
- L2: 속도와 용량의 절충이다.
- L3: 여러 코어가 공유하는 경우가 많다. 상대적으로 느리지만 용량이 크다.

---

## 5.1.4 다중 코어에서 생기는 캐시 일관성 문제

<img width="701" height="608" alt="image" src="https://github.com/user-attachments/assets/e59e7cd7-628c-46b8-b9b4-27cb457acef0" />


코어가 하나일 때는 캐시가 메모리를 대신해 값을 들고 있어도 큰 문제가 없다. 하지만 코어가 여러 개면 상황이 복잡해진다. 각 코어가 자기 캐시를 가지고 있으면, 같은 메모리 주소에 대한 값이 각 코어 캐시에 각각 존재할 수 있다. 그런데 한 코어가 값을 수정하면 다른 코어의 캐시는 여전히 예전 값을 들고 있을 수 있다. 그러면 “같은 변수인데 코어마다 값이 다르게 보이는” 상황이 발생한다.

이것이 캐시 일관성 문제다. 프로그램 입장에서는 메모리에 변수 X가 하나인데, 실제로는 캐시 때문에 X의 복사본이 여러 개 존재하는 셈이다. 따라서 하드웨어는 “한 코어가 값을 바꿨을 때 다른 코어가 그 사실을 알게 만들거나, 기존 값을 무효화하거나, 최신 값을 가져오게 하는” 메커니즘이 필요하다.

이를 위해 CPU는 MESI 같은 캐시 일관성 프로토콜을 사용한다. 핵심은 각 캐시 라인이 어떤 상태인지 관리하면서, 다른 코어와 통신해 불일치가 생기지 않도록 조정하는 것이다.

- 다중 코어에서는 “캐시가 빠르다”만으로 끝나지 않는다.
- “빠르면서도 값이 일치해야 한다”가 추가 조건이 된다.

---

## 5.1.5 캐시 갱신 정책: write-through vs write-back

캐시는 읽기뿐 아니라 쓰기에서도 문제가 생긴다. CPU가 어떤 값을 변경할 때, 그 변경을 어디에 먼저 반영할지 결정해야 한다. 이때 대표적인 방식이 두 가지다.

### write-through(연속 기입)

CPU가 캐시에 값을 쓰면, 동시에 메모리에도 바로 쓴다. 캐시와 메모리가 항상 같은 값을 유지하기 쉬워서 단순하다. 하지만 메모리 쓰기 자체가 느리기 때문에, 쓰기 성능이 떨어질 수 있다.

- 장점: 일관성 관리가 단순하다.
- 단점: 메모리 쓰기 때문에 느려질 수 있다.

### write-back(후기입)

CPU가 캐시에만 먼저 쓰고, 그 캐시 라인이 교체될 때(혹은 특정 시점에) 메모리에 반영한다. 메모리 쓰기 횟수를 줄여 성능이 좋다. 하지만 메모리가 “항상 최신 값”을 들고 있지 않을 수 있기 때문에 관리가 더 복잡해진다.

- 장점: 성능이 좋다.
- 단점: 관리가 복잡하고, 일부 상황에서는 메모리와 캐시 값이 다를 수 있다.

현대 CPU는 성능 때문에 보통 write-back을 사용한다.

---

## 5.1.6 메모리는 디스크를 캐시한다

캐시라는 개념은 CPU 내부에서만 끝나지 않는다. 운영체제 관점에서 보면 메모리 자체가 디스크를 위한 캐시 역할을 한다. 디스크는 메모리보다 훨씬 느리기 때문에, 파일을 읽을 때마다 디스크에 접근하면 성능이 급격히 떨어진다. 그래서 운영체제는 한 번 읽은 파일 데이터나 자주 쓰는 블록을 메모리에 보관해두고, 다음에 같은 데이터가 필요하면 디스크 대신 메모리에서 제공한다.

이때 중요한 것은 프로그램이 직접 “디스크 캐시를 써야지”라고 지시하지 않아도 된다는 점이다. 대부분 운영체제는 자동으로 페이지 캐시 같은 메커니즘을 사용해 디스크 접근을 줄인다.

- 메모리는 디스크에 비해 압도적으로 빠르다.
- 그래서 운영체제는 메모리를 디스크 캐시로 활용한다.

---

## 5.1.7 CPU는 어떻게 메모리를 읽는가

CPU가 보는 주소는 보통 가상 주소다. 프로그램은 자신만의 주소 공간을 가진다고 “생각하고” 실행되며, 실제 물리 메모리 위치는 운영체제와 하드웨어가 변환해준다. 이 변환을 담당하는 것이 MMU이고, 주소 변환이 끝나야 캐시에서 데이터를 찾거나 메모리에 접근할 수 있다.

즉 CPU가 메모리를 읽는 흐름은 다음처럼 이해하는 게 자연스럽다.

1. CPU가 가상 주소로 접근을 시도한다.
2. 주소 변환을 통해 물리 주소가 결정된다.
3. 캐시 계층에서 해당 데이터가 있는지 탐색한다.
4. 없으면 메모리에서 가져와 캐시에 채운 뒤 사용한다.
- CPU는 보통 “가상 주소로 생각하고 접근한다.”
- 실제 접근은 “주소 변환 + 캐시 탐색”을 거쳐 진행된다.

---

## 5.1.8 분산 저장에서도 캐시는 계속 등장한다

데이터가 너무 커지면 한 대의 컴퓨터 디스크에 담을 수 없고, 여러 서버에 나누어 저장하는 분산 파일 시스템을 사용하게 된다. 이때도 캐시 계층 구조는 그대로 등장한다. 로컬 메모리와 로컬 디스크는 분산 저장소의 캐시처럼 동작하고, 네트워크 너머의 원격 저장은 더 느리지만 더 큰 계층이 된다.

즉 저장 시스템은 아래로 내려갈수록 느려지고 크기가 커지는 “계층형 구조”로 확장된다.

레지스터 → L1/L2/L3 → 메모리 → 디스크 → 분산 파일 시스템

이 구조를 이해하면 “캐시는 CPU 성능 최적화 기술”이라는 좁은 관점이 아니라, “컴퓨터 시스템 전체의 성능을 성립시키는 보편적 원리”로 보이게 된다.

- 캐시는 어디에나 존재한다.
- 느린 것을 숨기기 위해 더 빠른 계층을 앞에 둔다는 발상은 시스템 전반에 반복된다.
