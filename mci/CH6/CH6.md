## 6.3. 파일 읽기: 메모리 관점의 I/O

### 6.3.1. 메모리 관점에서의 입출력

- **개념:** 입출력은 본질적으로 메모리와 외부 장치 사이의 데이터 복사
- **방향:**
    - **Input:** 외부 장치 $\rightarrow$ 메모리로 데이터 복사
    - **Output:** 메모리 $\rightarrow$ 외부 장치로 데이터 복사

### 6.3.2. Read 함수가 파일을 읽는 원리

- **시스템 콜 요청:** CPU가 OS 커널에 읽기 요청을 보냄
- **DMA(Direct Memory Access) 작동:** CPU가 복사에 매달리지 않도록, DMA 컨트롤러가 디스크에서 메모리로 데이터를 직접 복사
- **프로세스 상태 제어:**
    - **Blocking:** I/O 요청을 한 프로세스 A는 대기열로 이동하여 잠든다
    - **Scheduling:** CPU는 유휴시간 없이 준비 완료 대기열의 다른 프로세스(B)를 실행
    - **Interrupt:** 복사가 완료되면 인터럽트 신호를 보내 프로세스 A를 깨움
- **복사 방식의 차이:**
    - **우회:** 커널 메모리에 먼저 복사 후 사용자 공간으로 전달.
    - **직접(Zero-copy):** 중간 단계 없이 사용자 주소 공간으로 바로 복사하여 오버헤드를 줄임.

---

## 6.4. 높은 동시성의 비결: 입출력 다중화

### 6.4.1. 파일 서술자 (File Descriptor)

- **“Everything is a file”:** 유닉스 계열 시스템은 디스크, 네트워크, 터미널 등 모든 I/O 장치를 파일로 관리
- **FD의 역할:** `open()` 시 할당되는 정수 식별자로, 커널이 어떤 장치와 통신할지 결정하는 번호표

### 6.4.2. 다중 입출력 처리의 한계

- **핵심 문제:** 높은 동시성을 확보하기 위해 서버가 여러 사용자 요청을 처리해야 할 때, 기존 방식은 `read()` 호출 시 데이터가 올 때까지 프로세스가 멈추는(Blocking) 문제가 발생
    
    결론적으로는 FD에 대응하는 외부 장치가 읽고 쓸 상태인지 미리 알 수가 없기 때문. FD1을 확인하는 동안 FD2에 데이터가 도착했음을 알 방법은?
    
- **멀티스레딩의 한계:** 사용자 요청마다 스레드를 생성하면 메모리 소모와 스케줄링(Context Switching) 부하가 심화됨

### 6.4.3. 비동기적 접근 (전화기 비유)

- **문제 상황:** 외부 장치가 언제 데이터를 보낼지 알 수 없어 무작정 기다려야 하는 상황.
- **해결책:** 커널이 FD를 지켜보고 있다가 데이터가 도착했을 때 응용 프로그램에 알려주는 구조

### 6.4.4. 입출력 다중화 (Multiplexing)

- **원리:** 여러 개의 입력 신호를 하나의 통로로 모으는 하드웨어 MUX 기술을 I/O에 적용함.
- **단계:** 1. 소켓 FD 획득 $\rightarrow$ 2. 커널에 감시 함수 호출 $\rightarrow$ 3. 준비된 FD 목록 반환.
    
    참고로 비동기적 접근이라고 하지만 응용 프로그램은 커널에 이벤트 트리거된 FD 목록을 받기 전까지 블로킹됨. 비동기적이라고 하는 이유는 **어떤 FD에서 이벤트가 발생할지 모르는 상태에서 무작정 하나의 I/O에 묶여(Blocking) 있지 않아도 되기 때문(논리적 비동기성)**
    
    **$\rightarrow$  "단일 I/O의 블로킹에 갇히지 않고 여러 이벤트를 동시에 처리할 수 있는 구조(Event-driven)"**
    

### 6.4.5. select, poll, epoll의 차이

- **`select`:** 최대 1024개의 FD만 감시 가능. 매번 전체 리스트를 순회하며 확인해야 함 ($O(N)$).
- **`poll`:** FD 개수 제한은 없지만, 여전히 전체 감시 대상을 훑어야 하는 성능 한계가 존재
- **`epoll`:** 커널이 '준비 완료된 FD 목록'만 별도로 관리하여 반환. 대규모 동시 접속 처리에 최적화된 방식

---

## 6.5. mmap: 메모리 읽고 쓰듯 파일 처리하기

디스크와 메모리는 특정 주소 지정 방법이 다르기에 특별한 처리 필요.

### 6.5.1. 파일과 가상 메모리

- **가상 메모리 목적:** 모든 프로세스가 각자 독립적/독점적으로 메모리를 소유하고 있다는 환상을 제공
- **mmap의 정의:** 디스크의 파일 영역을 프로세스의 가상 메모리 주소 공간에 직접 사상(Mapping)

### 6.5.2. 직접 사상(Direct Mapping)의 메커니즘

- **동작 방식:** 파일의 특정 섹션이 메모리 주소(예: 600~799)에 대응되도록 설정
- 디스크 주소 공간(600-799) 접근 $\rightarrow$ Page Fault Interrupt 발생 $\rightarrow$  실제 디스크 입출력
    1. **주소 접근:** 프로세스가 특정 가상 주소에 접근
    2. **Page Fault Interrupt:** 해당 데이터가 실제 메모리에 로드되지 않은 경우 인터럽트가 발생
    3. **자동 I/O:** OS가 디스크에서 필요한 부분만 메모리에 올림
- **투명성:** 개발자는 복잡한 함수 없이 일반 메모리를 읽고 쓰듯 파일을 제어하게 됨.

### 6.5.3. mmap VS 전통적인 read/write

- **mmap 단점**: 무작정 좋지는 않음.
    1. 사상 유지하기 위해 특정 데이터 구조 사용해야 함
    2. Page Fault Interrupt 처리에 대한 부담
- **read/write 단점**: user 모드와 kernel 모드를 오가면서 시스템에 부담을 줌(시스템 호출 + 메모리 복사)

### 6.5.4. 큰 파일 처리

- **장점:** 물리 메모리 용량을 초과하는 아주 큰 파일도 가상 메모리 주소 공간에 직접 사상하여 처리 가능. (기존 read/write 함수 사용 시 부분적으로 메모리 요청, 이때 메모리 요청이 너무 커질 경우 **메모리 부족 강제 종료(out of memory killer)** 일으킬 수 있음)
- **동작 원리:** 물리 메모리보다 큰 파일을 매핑하더라도 시스템이 가상 메모리를 통해 필요한 부분만 적절히 이동시키므로, 개발자는 복잡한 메모리 관리에 신경 쓰지 않고 코드를 단순화할 수 있다.
    
    $\rightarrow$ 가상메모리 연결만 해두고 ‘실제로 해당 메모리가 사용될 때’ 그 부분만 RAM으로 가져온다. (Lazy Loading과 비슷)
    
    1. 주소 공간 사상 (Mapping)
        - **직접 사상의 의미:** 디스크 파일의 내용을 물리 메모리(RAM)에 미리 복사하는 것이 아니라, 프로세스의 **가상 주소 공간**에 연결만 하는 것
        - **자원 절약:** `mmap` 호출 시점에는 시스템 콜 오버헤드와 데이터 복사 부하가 거의 발생 x
    2. 물리 메모리 한계 극복
        - **가상 메모리 활용:** 프로세스 주소 공간(64비트 기준)은 실제 RAM보다 훨씬 크기 때문에, 수십 GB의 파일도 주소 공간에 통째로 올릴 수 있음.
        - **투명한 데이터 로딩:** 프로그램이 매핑된 주소에 접근할 때 **페이지 폴트(Page Fault)**가 발생하며, OS가 필요한 부분만 메모리에 적재함.
        - **코드 단순화:** 개발자는 파일의 어느 부분을 `read`하고 `seek`할지 고민할 필요 없이, 거대한 메모리 포인터를 다루듯 코드를 작성할 수 있음.
    3. 매개변수에 따른 동작 변화
        
        
        | **플래그** | **메모리 할당 및 반영 방식** | **주요 용도** |
        | --- | --- | --- |
        | **`MAP_SHARED`** | 수정 내용이 실제 디스크 파일에 반영됨. 시스템 메모리를 실제로 공유함. | 프로세스 간 통신(IPC), 파일 수정 |
        | **`MAP_PRIVATE`** | 수정 시 별도의 복사본을 생성(Copy-on-Write). 원본 파일은 유지됨. | 실행 파일(.exe, .so) 로딩 |
    4. 주의사항 (6.5.5 연결)
        - **32비트 시스템의 제약:** 프로세스 주소 공간 자체가 4GB로 제한되므로, 파일 크기가 이보다 크면 `mmap` 호출이 실패할 수 있음.
        - **성능 측정의 필요성:** 항상 `mmap`이 `read/write`보다 빠른 것은 아니며, 랜덤 액세스가 빈번하거나 페이지 폴트 비용이 큰 경우 직접 테스트가 필요함.

### 6.5.5. 동적 링크 라이브러리(DLL)와 공유 메모리

- **핵심 내용**: `mmap`의 메모리 절약 - 실제 RAM 공간은 하나만 쓰면서, 여러 프로그램이 각자 자기 것인 양 나눠 쓰게 해주는 연결고리 역할.
- **정적 라이브러리와 동적 라이브러리의 차이**
    
    
    | **구분** | **정적 라이브러리 (Static)** | **동적 라이브러리 (Dynamic/Shared)** |
    | --- | --- | --- |
    | **방식** | 컴파일 시점에 라이브러리 코드를 내 실행 파일(.exe) 안에 **통째로 복사**해서 넣습니다. | 실행 파일에는 "이 라이브러리가 필요함"이라는 **정보**만 남기고, 실행될 때 연결합니다. |
    | **용량** | 프로그램 10개를 만들면 라이브러리 코드도 10번 중복 저장됩니다. 파일 크기가 커집니다. | 라이브러리 파일 하나만 디스크에 있으면 여러 프로그램이 공유하므로 파일 용량이 작습니다. |
    | **메모리** | 프로그램 10개를 실행하면 RAM에도 동일한 라이브러리 코드가 10번 올라갑니다. | **`mmap`** 덕분에 RAM에 딱 **하나**만 올려서 모두가 나눠 씁니다. |
1. **중복의 문제**
    - `libc`(C 표준 라이브러리) 같은 필수 라이브러리가 100MB라고 가정하면, 정적 링크를 쓰면 프로그램 10개를 실행할 때 RAM을 1GB나 소모하게 되어 자원 낭비가 심각함.
2. **mmap을 통한 해결 (공유 메모리)**
    - **물리 메모리 점유:** OS는 동적 라이브러리 파일을 RAM의 특정 영역에 딱 한 번만 로드
    - **가상 주소 매핑:** 각 프로세스는 자신의 주소 공간에 이 라이브러리가 들어갈 자리를 예약하고, `mmap`을 통해 **동일한 실제 RAM 주소**를 가리키도록 설정
3. **결과 및 이점**
    - **자원 절약:** 각 프로세스는 라이브러리가 자기 주소 공간에 통째로 들어와 있다고 생각하지만, 실제 RAM에는 단 하나의 복사본만 존재합니다.
    - **공유 메모리:** 이렇게 하나의 물리 메모리를 여러 프로세스가 동시에 사상하여 사용하는 방식을 통해 디스크 공간과 RAM을 획기적으로 절약합니다.
4. **실제 확인 (`strace` 명령어 활용)**
    - 리눅스에서 `ls` 같은 간단한 명령어를 실행할 때도 `strace`로 추적해보면, `libc.so` 같은 라이브러리를 `mmap`으로 주소 공간에 사상하는 과정을 직접 확인할 수 있음.
    - 거의 모든 현대적 프로그램은 실행 시 이 과정을 거쳐 표준 라이브러리를 적재함.

---

## 6.6. 컴퓨터 시스템의 지연 시간(Latency) 이해

제프 딘(Jeffrey Dean)이 언급한 통계를 바탕으로, 컴퓨터 시스템의 각 부분에서 발생하는 지연 시간을 분석함.

### 6.6.1. 시간 지표로의 환산

컴퓨터의 매우 빠른 시간($ns$)을 체감하기 위해 **L1 캐시 접근 시간(0.5ns)을 1초로 환산**했을 때의 상대적 속도는 아래와 같음.

| **작업 내용** | **실제 시간** | **0.5ns를 1초로 환산 시** |
| --- | --- | --- |
| **L1 캐시 참조** | 0.5 ns | 1초 |
| **분기 예측 오류** | 5 ns | 10초 |
| **L2 캐시 참조** | 7 ns | 14초 |
| **주 메모리(RAM) 참조** | 100 ns | 3분 |
| **SSD에서 1MB 순차 읽기** | 1,000,000 ns | 20일 |
| **디스크 탐색(Seek)** | 10,000,000 ns | 200일 |
| **인터넷 패킷 왕복(미국-네덜란드)** | 150,000,000 ns | 10년 |
| **시스템 재시작(Reboot)** | 120,000,000,000 ns | 7600년 |

### 6.6.2. 거리 지표로의 환산

**0.5ns를 1m로 환산**하여 물리적 거리에 비유하면 시스템 내부의 차이가 더 명확해짐.

- **L1 캐시 접근:** 집 안에서 문 앞의 택배를 가져오는 거리 (약 1m).
- **메모리(RAM) 접근:** 집에서 편의점에 다녀오는 거리 (약 200m).
- **SSD에서 1MB 읽기:** 서울에서 홍콩까지 가는 거리 (약 2000km).
- **디스크에서 1MB 읽기:** 지구를 한 바퀴 도는 거리 (약 4만km).
- **시스템 재시작:** 지구에서 화성까지 가는 거리 (약 2억 4천만km).
