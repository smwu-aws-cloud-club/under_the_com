# 5. 작은 것으로 큰 성과 이루기, 캐시

## 5.1 캐시, 어디에나 존재하는 것

일반적인 시스템에서 메모리의 속도는 CPU의 100분의 1에 불과하다.<br>
CPU와 메모리 사이의 불일치 문제를 해결하기 위해 캐시 계층을 추가한다.<br>
캐시 안에는 최근에 메모리에서 얻은 데이터가 저장되며, CPU는 메모리에서 명령어와 데이터를 꺼내야 할 때도 무조건 먼저 캐시에서 해당 내용을 찾는다.<br>
캐시가 적중하면 메모리에 접근할 필요가 없어 CPU가 명령어를 실행하는 속도를 크게 끌어올리는 목적을 달성할 수 있고, CPU와 메모리 사이의 속도 차이를 보완할 수 있다.

CPU와 메모리 사이에는 L1, L2, L3 세 단계의 캐시가 추가되어 있다.<br>
L1 캐시, L2 캐시, L3 캐시, CPU 코어는 레지스터 칩 내에 묶여 패키징되어 있다.<br>
CPU는 메모리에 접근할 필요가 있을 때 L1 캐시 → L2 캐시 → L3 캐시 → 메모리 순서로 접근한다.

캐시가 있기 때문에 메모리는 더 이상 메모리에 접근하지 않으며, CPU는 캐시에 직접 기록한다.<br>
이때 캐시의 데이터는 갱신되었지만 메모리의 데이터는 예전 데이터가 남아 있는 **불일치** 문제가 발생할 수 있다.<br>
이 문제를 해결하는 가장 간단한 방법은 캐시를 갱신할 때 메모리도 함께 갱신하는 **연속 기입** 방법이다.<br>
이 방식으로 캐시를 업데이트하면 CPU는 메모리가 갱신될 때까지 대기하고 있어야 하는 동기실 설계 방법에 해당한다.<br>
CPU가 메모리에 기록할 때는 캐시를 직접 갱신하지만, 메모리 갱신 완료를 기다릴 필요 없이 CPU는 다음 명령어를 실행할 수 있다.<br>
캐시 용량에 한계가 있어 용량이 부족하면 자주 사용되지 않는 데이터를 제거하는데, 이때 캐시에서 제거된 데이터가 수정된 적이 있다면 메모리에 갱신하는 비동기 방식의 **후기입** 방법을 사용할 수 있다. 이 방법은 연속 기입보다 훨씬 복잡하지만 성능은 더 낫다.

CPU는 다중 코어 시대에 진입했다. 다중 코어를 최대한 활용하는 다중 스레드 또는 다중 프로세스가 없다면 다중 코어의 위력을 충분히 활용할 수 없다.<br>
CPU가 코어 여러 개를 가지고 있다면 각 코어에서는 다른 스레드가 실행된다. 이 스레드들이 모두 메모리 내 변수 X에 접근하고 이 내용은 캐시에 저장된다.<br>
CPU는 각각 캐시를 갖기 때문에 메모리의 X 변수에 대해 각 캐시에 복사본을 가지고 있다. 이때 캐시에 저장된 변수에 대해 캐시들이 연결되어 있지 않으므로 값의 불일치 문제가 발생할 수 있다.<br>
이 문제를 해결하기 위해 캐시 한 개에서 갱신된 변수가 다른 CPU 코어의 캐시에도 함께 갱신되어야 하므로 다중 코어 캐시의 일관성을 유지하면 성능 문제가 발생한다.

파일을 읽을 때 먼저 데이터를 디스크에서 메모리로 옮겨야 CPU가 메모리에서 파일의 데이터를 읽을 수 있다.<br>
메모리와 디스크의 속도 차이를 해결하기 위해 메모리를 디스크의 캐시로 사용할 수 있다.<br>
컴퓨터 시스템의 여유 메모리 공간을 디스크의 캐시로 활용하여 디스크에서 데이터를 읽어 오는 일을 최소화한다.<br>
이것은 리눅스 운영 체제에서 페이지 캐시의 기본 원리에 해당한다.<br>
최근 메모리가 점점 저렴해지면서 RAM과 같은 메모리가 디스크를 대체하는 추세이다. Spark를 사용하는 등 일부 상황에서는 데이터베이스를 전부 메모리에 직접 설치하여 디스크 입출력이 필요 없는 경우도 있다.<br>
하지만 메모리는 데이터를 영구적으로 저장할 수 있는 기능이 없기 때문에 메모리가 디스크를 완전히 대체할 수는 없다.

프로세스 주소 공간의 데이터는 디스크로 대체될 수 있으므로 프로그램이 디스크 입출력을 포함하고 있지 않더라도 메모리 사용률이 매우 높을 경우에는 CPU가 디스크에 접근해야 할 수 있다.

CPU가 메모리를 사용할 때 실행하는 읽기와 쓰기 명령어가 사용하는 것은 가상 메모리 주소이므로 이 주소를 실제 물리 메모리 주소로 변환해야 한다.<br>
변환이 완료되면 캐시를 검색해 캐시에서 데이터를 찾는다면 직접 반환하며, 찾을 수 없을 때는 메모리에 접근한다.<br>
가상 메모리의 존재로 프로세스의 데이터는 디스크에 임시로 보관되어 있을 수 있는데, 해당 데이터를 메모리에서도 찾을 수 없다면 디스크의 프로세스 데이터를 다시 메모리에 적재한 후 메모리를 읽어야 한다. 

대용량의 데이터 저장 문제를 해결하기 위해 여러 대의 저장 장치를 사용하는 것을 분산 파일 시스템이라고 한다.<br>
사용자 장치는 분산 파일 시스템을 직접 장착(mount)할 수 있고, 로컬 디스크는 원격의 분산 파일 시스템에서 전송된 파일을 저장한다.<br>
분산 파일 시스템은 네트워크를 통하지 않고 로컬 디스크에 직접 접근하므로 로컬 디스크를 원격 분산 파일 시스템의 캐시로 간주할 수 있다.<br>
응답 속도를 더 높이기 위해 원격 분산 파일 시스템의 데이터를 data stream 형태로 직접 로컬 컴퓨터 시스템의 메모리로 끌어올 수도 있다.<br>
메시지 미들웨어인 apache kafka 시스템의 경우 대용량 메시지는 원격 분산 파일 시스템에 저장되어 있지만 실시간으로 해당 데이터의 소비자에게 전달하는데, 이 경우 메모리를 원격 분산 파일 시스템의 캐시로 간주할 수 있다.

![최신 컴퓨터 시스템의 저장 체계](/esc/CH5/images/cs_storage_system.png)<br>
컴퓨터 저장 체계의 각 계층이 다음 계층에 대한 캐시 역할을 한다.<br>
각 계층의 저장 용량은 반드시 다음 계층보다 작아야 한다.<br>
이에 근거하여 전체 저장 체계가 최대 성능을 발휘하려면 프로그램이 매우 캐시 친화적이어야 한다.

<br>

## 5.2 어떻게 캐시 친화적인 프로그램을 작성할까?

최신 컴퓨터 시스템에서는 프로그램이 메모리에 접근할 때 캐시의 적중률이 매우 중요하므로, 캐시 친화적인 프로그램은 **캐시 적중률이 높은 프로그램**이라고 할 수 있다.

프로그램 지역성의 원칙의 본질은 프로그램이 매우 규칙적으로 메모리에 접근한다는 것이다.<br>
프로그램이 메모리 조각에 접근하고 나서 이 조각을 여러 번 참조하는 경우를 **시간적 지역성**이라고 한다.<br>
시간적 지역성은 데이터가 캐시에 있는 한 메모리에 접근하지 않아도 반복적으로 캐시의 적중이 가능하므로 캐시 친화성이 높다.<br>
프로그램이 메모리 조각을 참조할 때 인접한 메모리도 참조할 수 있는데, 이를 **공간적 지역성**이라고 한다.<br>
캐시 적재 시에 메모리의 인접 데이터도 함께 저장되므로 공간적 지역성 역시 인접 데이터 접근 시 캐시 적중해 캐시 친화성이 높다.

메모리 풀 기술은 일반적으로 고성능 요구 사항이 있을 때만 사용된다.<br>
메모리 동적 할당 시 malloc을 사용해서 할당받으면 메모리 조각이 힙 영역에 흩어져 잇을 가능성이 높기 때문에 공간적 지역성이 좋지 않다.<br>
메모리 풀 기술은 연속적인 메모리 조각을 미리 할당받아 사용해 공간적 지역성을 높여 캐시의 적중률을 높였다.

연결 리스트의 경우에도 자주 사용되는 값을 인접하게 배치하면 공간적 지역성 원리로 구조체의 형태를 최적화할 수 있다.<br>
연결 리스트 자체가 차지하는 저장 공간이 클수록 캐시에 저장할 수 있는 노드는 줄어들기 때문에 접근 횟수가 높은 포인터와 value는 구조체에 넣고, 접근 횟수가 낮은 배열은 다른 구조체에 넣을 수 있다.<br>
여기에서 구조체의 배열은 cold data이며, 포인터와 value는 hot data이다. 이 데이터를 서로 분리하면 더 나은 지역성을 얻을 수 있다.<br>
지역성 원칙 관점에서는 배열이 연결 리스트보다 낫다. 배열은 하나의 연속된 메모리 공간에 할당되지만, 연결 리스트는 일반적으로 흩어져 있을 수 있기 때문이다.<br>
하지만 노드가 빈번하게 추가되고 삭제될 때는 연결 리스트가 배열에 비해 우수하므로 실제로 사용할 때는 구체적인 상황에 맞춰 선택해야 한다.<br>
연결 리스트를 생성할 때 직접 정의한 메모리 풀에서 메모리를 요청하면 각 연결 리스트 노드의 메모리 분포가 비교적 간결해지기 때문에 더 나은 공간적 지역성을 보여준다.

<br>

## 5.3 다중 스레드 성능 방해자

공간적 지역성 원리를 위해 프로그램이 접근한 데이터의 인접한 데이터까지 '묶음' 데이터로 캐시에 저장하며, 이 묶음 데이터를 캐시 라인이라고 부른다.

두 개의 스레드로 전역 변수 값을 1씩 5억 번 증가시키는 프로그램과 단일 스레드로 전역 변수 값을 1씩 10억 번 증가시키는 프로그램 중 후자가 더 빨리 실행된다.<br>
병렬 계산임에도 다중 스레드가 단일 스레드보다 느린 이유가 무엇일까?<br>
다중 스레드 프로그램의 insn per cycle(클럭 주기 당 명령어 실행 횟수)는 0.15로, 단일 스레드 프로그램의 0.6보다 4개 정도 낮다.

다중 스레드 프로그램은 캐시 일관성을 보장하기 위해 두 코어의 캐시 C1, C2에 전역 변수를 저장한다.<br>
이때 첫 번째 스레드에서 변수에 덧셈을 했다면 반드시 C2 캐시의 변수를 무효화해야 하는데, 여기서 첫 번째 캐시 튕김이 발생한다.<br>
C2는 메모리에서 직접 변수 값을 읽어와 변수에 덧셈을 하고, C1 캐시는 C2에서 작업이 일어났으므로 변수 무효화를 위한 캐시 튕김이 발생한다.<br>
이렇게 C1 캐시와 C2 캐시는 끊임없이 서로 상대 캐시를 무효화하면서 튕겨낸다. 이로 인해 캐시가 자신의 역할을 하지 못할 뿐만 아니라 프로그램 성능을 저하시킨다.<br>
따라서 여러 스레드 사이에 데이터 공유를 피할 수 있다면 가능한 한 피하는 것이 좋다.

이번에는 병렬 스레드와 단일 스레드로 구조체의 a 변수와 b 변수를 1씩 각각 5억 번 증가시키는 프로그램의 성능을 비교해보자.<br>
이번에는 두 스레드가 변수를 공유하지 않으므로 캐시 튕김 문제가 없을 것이라고 생각해 다중 스레드 프로그램이 더 빠르게 실행될 것이라고 생각할 수 있다.<br>
하지만 단일 스레드 프로그램이 더 빠르게 실행된다. 왜냐하면 두 변수가 동일한 캐시 라인을 공유하고 있기 때문이다.
a 변수를 캐시에 적재할 때 근처에 있는 b 변수도 캐시에 함께 저장된다. 따라서 캐시의 동작 방식에 따라 캐시 라인을 공유하는데, 이런 문제를 **거짓 공유**라고 부른다.<br>
거짓 공유 문제는 두 변수 사이에 사용되지 않는 데이터를 채우거나 구조체 내 변수 순서를 조정해 해결할 수 있다.

<br>

## 5.4 봉화희제후와 메모리 장벽

원래 CPU는 명령어를 비순차적으로 실행한다.
1. 기계 명령어를 생성하는 단계: 컴파일 중에 명령어를 재정렬한다.
2. CPU가 명령어를 실행하는 단계: 실행 중에 명령어가 비순차적으로 실행된다.

<br>

```
int a; int b;

void main() {
    a = b + 100;
    b = 200;
}
```
위의 코드에 대해 컴파일러가 코드를 최적화할 수 있도록 하는 -O2 옵션을 적용하여 컴파일을 진행하면 b=200 코드 줄을 a = b+100 코드 줄 앞에 놓는다.<br>
이렇게 컴파일러는 명령어를 재정렬하는 동작을 수행할 수 있다.

또한 CPU가 명령어를 실행할 때도 __비순차적(Out of Order Execution, OoOE)__ 으로 실행한다.<br>
순차적으로 실행할 경우 필요한 피연산자가 아직 준비되지 않은 경우 CPU가 반드시 대기해야 하므로 비효율적이기 때문이다.<br>
비순차적 명령어 실행 기능은 CPU가 대기하며 발생하는 파이프라인의 빈 공간을 메꿔 명령어 실행 속도를 높인다.
1. 기계 명령어를 가져온다.
2. 명령어를 대기열에 넣고 명령어에 필요한 피연산자를 읽는다.
3. 명령어는 대기역에서 피연산자의 준비가 완료될 때까지 대기한다. 준비가 완료된 명령어가 먼저 실행 단계에 들어갈 수 있다.
4. 기계 명령어를 실행하면 실행 결과를 대기열에 넣는다.
5. 이전 명령어의 실행 결과가 기록될 때까지 기다렸다가 현재 명령어의 실행 결과를 기록한다. (원래 실행 순서에 따른 결과를 얻기 위함)

이는 전후 관계의 명령어가 서로 어떤 의존 관계도 없을 때 사용 가능하며, 모든 CPU가 이 기능을 가지고 있지는 않다.

L1 캐시와 L2 캐시는 각 CPU 코어마다 별도로 존재하며, L3 캐시와 메모리는 모든 코어가 공유한다.<br>
캐시가 있는 시스템은 캐시를 갱신하고 일관성을 유지시키는 문제에 맞닥뜨린다.<br>
이 과정을 최적화하기 위해 일부 시스템은 저장 버퍼 등 대기열을 추가하고 기록 작업이 있을 때 대기열에 기록하고 캐시는 즉시 갱신하지 않는다.<br>
CPU는 기록 작업이 실제로 캐시와 메모리를 갱신할 때까지 기다리지 않고 다음 명령어를 실행하는 비동기 기록 작업을 한다.<br>
CPU 내부에서는 이전 명령어의 실행 결과가 기록될 때까지 기다렸다가 현재 명령어의 실행 결과를 기록하므로 해당 스레드에서는 확인할 수 없고, 다른 코어가 해당 코어를 바라볼 때만 비순차적 실행을 확인할 수 있다.

잠금 없는 프로그래밍은 잠금을 통한 보호를 사용하지 않는 상태에서 다중 스레드의 공유 리소스를 처리한다.<br>
일반적으로 다중 스레드에서 공유 리소스를 사용하는 것은 잠금을 통한 보호가 필요하다고 얘기하지만, 반드시 잠금이 필요한 것은 아니고 CAS 알고리즘과 같은 원자성 작업을 사용하면 괜찮다.

명령어의 비순차적 실행 문제는 **메모리 장벽**을 통해 해결할 수 있다.<br>
명령어는 비순차적으로 실행될 수 있지만, 메모리 장벽 기계 명령어를 통해 해당 스레드에 CPU 코어에 대해 비순차적 실행을 금지한다.<br>
메모리는 읽기(Load)와 쓰기(Store)만 존재하므로 LoadLoad, StoreStore, LoadStore, StoreLoad 네 가지 메모리 장벽 유형이 존재한다.

- LoadLoad: 2번째 Load 명령어가 먼저 실행되는 것을 방지함 - 변수에 저장된 예전 값을 읽지 않도록 보장함
- StoreStore: 2번째 Store 명령어가 먼저 실행되는 것을 방지함 - 변수의 갱신 순서와 코드 순서 일치시킴
    - 다른 순서가 즉시 최신 값을 확인하는 것을 보장 X
- LoadStore: Load가 캐시 적중 실패 시 Store가 먼저 실행되는 것을 방지함
- StoreLoad: Store 명령어를 실행할 때 Load를 먼저 실행하는 것을 방지함
    - 메모리 장벽 중 가장 무거움
    - StoreLoad 메모리 장벽 이후에 읽으면 그 값이 반드시 최신 값임을 보장함
    - 본질적으로 동기 작업

<br>

획득-해제 의미론을 통해 스레드 간 동기화 문제를 해결할 수 있다.<br>
획득 의미론은 메모리 읽기 작업에 대한 것으로, Load 뒤에 있는 모든 메모리 작업은 Load 작업 이전에는 실행이 불가능하다.<br>
해제 의미론은 메모리 쓰기 작업에 대한 것으로, Store 앞에 있는 모든 메모리 작업은 Store 작업 이후에는 실행이 불가능하다.<br>
획득-해제 의미론에 따르면 LoadLoad와 LoadStore 조합은 획득 의미론이고, StoreStore와 LoadStore 조합은 해제 의미론이다.<br>
또한 획득-해제 의미론을 얻기 위해 StoreLoad처럼 무거운 메모리 장벽이 필요하지 않으며, 나머지 세 종류의 메모리 장벽만 사용하면 된다.<br>
먼저 실행되는 스레드 1과 다음에 실행되는 스레드 2가 있다고 할 때 각 스레드에는 다음과 같은 의미론을 적용하면 된다.
- 스레드 1: 헤제 의미론, 모든 메모리의 읽기와 쓰기 작업을 신호를 설정한 후 하지 않음
- 스레드 2: 획득 의미론, 신호를 받는 작업 이전에는 어떠한 작업도 실행하지 않음

<br>

이식성이 높은 잠금 없는 프로그래밍을 하고 싶다면 언어 수준에서 제공하는 획득-해제 의미론을 사용해야 한다.<br>
다중 스레드의 공유 변수를 읽고 쓸 때 원자 변수로 선언해야 하며, 이것으로 다른 스레드의 변수가 변경 중인 상태를 보지 못하도록 해야 한다.<br>
x86에는 mfence 명령어를 사용해 StoreLoad 재정렬만 발생할 수 있는 것처럼 모든 유형의 CPU에 명령어 재정렬이 있는 것은 아니다.<br>
Alpha, ARMv7 등 모든 유형의 명령어 재정렬을 제공하는 플랫폼을 약한 메모리 모델이라고 부른다.<br>
x86처럼 StoreLoad 재정렬만 있는 플랫폼을 강한 메모리 모델이라고 부른다.

잠금 없는 프로그래밍을 해야 할 때만 명령어 재정렬에 신경 쓰면 된다. 공유 변수가 잠금 보호 없이 여러 스레드에서 사용되기 때문에 발생하는 문제이기 때문이다.<br>
다중 스레드 프로그래밍에서 일반적으로 잠금은 공유 변수를 보호하는 데 사용된다.<br>
하지만 잠금을 유지하는 스레드가 운영 체제에 의해 일시 중지된다면 잠금이 필요한 모든 다른 스레드의 동작도 멈추게 된다.<br>
잠금 없는 프로그래밍은 운영 체제가 순서 스케줄링을 어떻게 해도 일단 하나의 스레드는 무조건 실행할 수 있다. 이런 특징을 가지고 있는 운영 체제를 **잠금 없음**이라고 부른다.<br>
잠금을 사용하여 프로그래밍을 하면 잠금이 명령어 재정렬 문제를 자동으로 처리한다.

다중 스레드 프로그래밍에서 공유 리소스를 보호하기 위해 일반적으로 __상호 배제(mutual exclusion)__ 를 사용한다.<br>
동시에 최대 하나의 스레드만 상호 배제를 보유할 수 있으며, 해당 잠금이 사용되면 해당 잠금을 요청하는 다른 스레드는 운영 체제에 의해 대기 상태로 진입한다. 이는 잠금을 사용한스레드가 잠금을 해제할 때까지 계속된다.<br>
이외에 잠금이 사용된 후 잠금을 요청하는 스레드는 계속 잠금 해제 여부를 반복적으로 확인한다. 이때 잠금을 요청하는 다른 스레드는 운영 체제에 의해 대기 상태로 진입하지 않으므로 **스핀 잠금**이라고 한다.<br>
잠금을 이용한 프로그래밍은 잠금을 사용하고 있을 때, 잠금을 요청하는 다른 스레드는 계속 대기 상태를 유지한다.

잠금 없는 프로그래밍은 어떤 스레드가 공유 자원을 사용하고 있을 때, 다른 스레드도 해당 공유 자원을 사용하기 위해 대기 상태로 만들지 않는다.<br>
공유 리소스가 사용되는 것을 감지하면 일단 다른 필요한 작업으로 넘어가는데, 이것이 잠금 프로그래밍과 잠금 없는 프로그래밍의 가장 큰 차이점이다.<br>
잠금 없는 프로그래밍은 시스템 성능 향상에 사용되지 않고 스레드가 항상 대기 없이 어떤 일을 하도록 하는 것에 가치를 두고 있다.<br>
이것은 실시간 요구 사항이 높은 시스템에는 매우 중요하지만, 잠금 없는 프로그래밍은 매우 많고 복잡한 리소스 경쟁 문제와 ABA 문제를 처리해야 하며 코드 구현이 복잡하다.<br>
명령어 재정렬 문제는 확실히 CPU 성능 향상에 도움이 되지만 매우 어렵고, 오히려 많은 종류의 버그를 일으키는 주요 원인이 될 수 있다.

1. 성능을 위해, CPU는 반드시 프로그래머가 코드를 작성한 순서대로 엄격하게 기계 명령어를 실행할 필요가 없다.
2. 프로그램이 단일 스레드인 경우 프로그래머는 명령어의 비순차적 실행을 볼 수 없으므로, 단일 스레드 프로그램은 명령어 재정렬에 신경 쓸 필요가 없다.
3. 메모리 장벽의 목적은 특정 코어가 명령어를 실행하는 순서와 다른 코어에서 보이는 순서가 코드 순서와 일치하도록 만드는 것이다.
4. 멀티 스레드 잠금 없는 프로그래밍을 사용할 필요가 없다면 명령어 재정렬을 걱정할 필요가 없다.

<br>